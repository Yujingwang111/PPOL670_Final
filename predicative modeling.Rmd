---
title: "Predicative Crime of D.C."
author: "Selina Sun"
date: "5/4/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#data wrangling and pre-processing
library(tidyverse)
library(plyr)
library(dplyr)
library(readr)
library(purrr)
library(lubridate)
library(hms)
library(janitor)
#mapping 
library(sf)
library(viridis)
library(tigris)
library(ggspatial)
library(patchwork)
#getting census tract data
library(tidycensus)
library(httr)
library(dotenv)
#machine
library(recipes)
library(tidymodels)
library(glmnet)
library(caret)
library(vip)
library(parsnip)
library(randomForest)
library(ranger)
library(rpart)
#time series analysis 
library(plotly)
library(TSstudio)
library(forecast)
library(MLmetrics)

#read and load DC crimes data from 2017-2021
crimes <-
  list.files(path = "data/",
             pattern = "*.csv", 
             full.names = T) %>% 
  map_df(~read_csv(., col_types = cols("CCN"= col_double(),
                                       "LONGITUDE" = col_character(),
                                       "LATITUDE" = col_character()))) 
```

#1.Crime data:data wrangling and pre-processing
```{r dc_crime}
#change case of the variables
change_case<-function(x){
  return(gsub("\\b([A-Z])([A-Z]+)", "\\U\\1\\L\\2", x, perl=TRUE))
}
#change case and split date variable to weekdays, month and year, label property offense 
crimes<-crimes%>%
  clean_names()%>%
  dplyr::mutate(shift=change_case(shift),
                method=change_case(method),
                offense=change_case(offense),
                block=change_case(block),
                bid=change_case(bid),
                weekday=lubridate::wday((report_dat), label=TRUE),
                month=lubridate::month((report_dat),label=TRUE),
                year=lubridate::year(report_dat),
                day=lubridate::day(report_dat),
                rep_date=date(report_dat),
                rep_time=hms::as_hms(ymd_hms(report_dat)),
                hours=lubridate::hour(rep_time),
                autotheft=ifelse(offense=="Theft F/Auto", 1, 0))%>%
  mutate(autotheft=as.factor(autotheft))%>%
  as_tibble()

#identify duplication incidents by CCN
crimes%>%
  group_by(ccn)%>%
  dplyr::mutate(count=n())%>%
  filter(count>1)
#remove duplicate rows 
crimes_clean<-filter(distinct(crimes, ccn, .keep_all = TRUE))
#check 
crimes_clean%>%
  group_by(ccn)%>%
  dplyr::mutate(count=n())%>%
  filter(count>1)
#overview of crime data
crimes_clean%>%
  group_by(offense)%>%
  dplyr::summarize(count=n())%>%
  arrange(desc(count))
#calculate frequency by offense 
crimes_clean%>%
  group_by(offense)%>%
  dplyr::summarise(count=sum(ccn, na.rm = TRUE))%>%
  mutate(freq=100*count/sum(count,na.rm = TRUE))%>%
  arrange(desc(freq))%>%
  ungroup()

#sf theft to join
crimes_sf <- st_as_sf(
  crimes_clean, 
  coords = c("longitude", "latitude"), 
  crs = 4326
)
```
In the table below, we observe that THEFT comprises about 70% of total crimes in the historical dataset. It represents over ??? observations from 2017 to 2021. This is a sufficiently rich dataset for predictive analysis.
#City limits
```{r}
#DC limits
#load the census tracts of dc
unzip(zipfile="data/Census_Tracts_in_2020.zip",
      exdir = "data")
#read shapefile into sf
dc_sf<-st_read("data/Census_Tracts_in_2020.shp")%>%
  clean_names()%>%
  select(geoid, geometry,tract)%>%
  st_transform(crs=4326)
```
#Census Tracts variables poverty, pop dens and unemployment
#### Getting Census Tracts through API
```{r}
readRenviron("~/.Renviron")
credential<- Sys.getenv("census_api_key")
#find variable code and select variables
v2020 <- load_variables(2020, "acs5", cache = TRUE)
#population B01003_001
#poverty B17017_001
#unemployment B23025_001
#household income B19013_001
#getting variables from acs
options(tigris_use_cache = TRUE)
dc_acs<-as_tibble(get_acs(
  geography = "tract",
  survey="acs5",
  variables = c("B01003_001","B17017_001","B17017_002", "B23025_001",
  "B23025_005", "B19013_001","B06009_001", "B06009_002"),
  state=11,
  county=001,
  geometry=TRUE,
  year=2020,
))
#data wrangling and preprocessing 
#pivot tibbles and rename variables to get demographic features of dc
dc_demo<-dc_acs%>%
  clean_names()%>%
  select(geoid, variable, estimate, geometry)%>%
  pivot_wider(
    names_from = "variable", 
    values_from = "estimate")%>%
  dplyr::rename(population=B01003_001,
                poverty_raw=B17017_001,
                poverty_below=B17017_002,
                employ=B23025_001,
                unemploy=B23025_005,
                income=B19013_001,
                educ=B06009_001,
                belowhigh=B06009_002)%>%
  #handling missing data with average values
  mutate(avg_poverty_rate=poverty_below/poverty_raw, #extract regional average poverty rate
         avg_poverty_rate=ifelse(poverty_raw == 0, 0, poverty_below/poverty_raw),#impute missing data
         avg_unemploy_rate=unemploy/employ,#extract regional average poverty rate
         avg_unemploy_rate=ifelse(employ==0, avg_unemploy_rate, unemploy/employ),#impute missing data
         income=ifelse(is.na(income) | income==0, 98654, income), #median household income for missing data
         population=ifelse(is.na(population), 0, population),
         below_high=belowhigh/educ, #regional below high school educ
         below_high=ifelse(is.na(educ), 0, belowhigh/educ))#impute missing 
```
#join census tract data with crime data 
```{r}
data<-st_join(crimes_sf, dc_sf, join=st_intersects)
data<-left_join(data, dc_demo, by="geoid")%>%
  st_set_geometry(NULL)
sample<-data%>%
  select(shift, ward, psa, neighborhood_cluster, 
         block_group, weekday,month,year,hours,
         autotheft,population,income, 
         avg_poverty_rate,avg_unemploy_rate,below_high)%>%
  mutate(autotheft=as.factor(autotheft))
write_csv(sample,"sample.csv")
```

```{r}
set.seed(20220502)
split<-initial_split(sample, prop=0.75)
theft_train<-training(split)
theft_test<-testing(split)

theft_rec<-
  recipe(autotheft ~ ., data = theft_train) %>%
  themis::step_downsample(autotheft)%>%
  prep()
# see the engineered training data
bake(prep(theft_rec, training = theft_train), new_data = theft_train)
# set up resampling using 10-fold cross validation
folds <- vfold_cv(data = theft_train, v = 10, repeats = 1)
```

```{r}
set.seed(20220502)
lg_rec<-
  recipe(autotheft ~ ., data = theft_train) %>%
  # collapse low-frequency categories
  step_other(neighborhood_cluster, shift) %>%
  # dummy encode categorical predictors
  step_dummy(all_nominal_predictors()) %>%
  # center and scale predictors
  step_center(income, population, avg_poverty_rate, avg_unemploy_rate, below_high) %>%
  step_scale(income, population, avg_poverty_rate, avg_unemploy_rate, below_high) %>%
  # drop near zero variance predictors
  step_nzv(all_predictors()) %>%
  # drop na
  step_naomit(all_predictors())
bake(prep(lg_rec, training = theft_train), new_data = theft_train)
```

```{r}
dt_mod<-
  decision_tree(cost_complexity = tune(),
                tree_depth=tune(),
                min_n=tune())%>%
  set_engine(engine = "rpart")%>%
  set_mode(mode="classification")

dt_wf<-workflow()%>%
  add_recipe(theft_rec)%>%
  add_model(dt_mod)

dt_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          min_n(), 
                          levels = 3)
dt_tuning<-dt_wf %>%
  tune_grid(resamples=folds,
            grid=dt_grid)
#plot the rmse for decision tree
collect_metrics(dt_tuning, summarize = FALSE)%>%
  filter(.metric == "rmse")%>%
  ggplot(aes(id, .estimate, group= .estimator))+
  geom_line()+
  geom_point()+
  scale_y_continuous(limits = c(0, 3)) +
  labs(title = "Calculated RMSE Across the 10 Folds",
     y = "RMSE_hat") +
theme_minimal()
dt_tuning%>%
  show_best(metric="rmse")
dt_best <- dt_tuning %>%
  select_best(metric="rmse")
dt_final <- finalize_workflow(
  dt_wf,
  parameters = dt_best
)
dt_final <- finalize_workflow(
  dt_wf,
  parameters = dt_best
)
dt_wf_fit<-dt_final%>%
  fit(data=theft_train)
dt_fit<-dt_wf_fit%>%
  pull_workflow_fit()
dt_coefs <- dt_final %>%
  fit(data = theft_train) %>%
  extract_fit_parsnip() %>%
  vi(lambda = dt_best)
vip(dt_fit)
```

```{r}
sampled_mod <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")
sampled_wf <- workflow() %>%
  add_recipe(theft_rec) %>%
  add_model(sampled_mod) 
#fit the model
sampled_fit <- sampled_wf %>%
  fit(data = theft_train)
#plot a decision tree
rpart.plot::rpart.plot(x = sampled_fit$fit$fit$fit)


predictions <- bind_cols(
  theft_test,
  predict(object = sampled_fit, new_data = theft_test),
  predict(object = sampled_fit, new_data = theft_test, type = "prob")
)
select(predictions, autotheft, starts_with(".pred"))

conf_mat(data = predictions,
         truth = autotheft,
         estimate = .pred_class)
yardstick::accuracy(data = predictions,
         truth = autotheft,
         estimate = .pred_class)
yardstick::recall(data = predictions,
       truth = autotheft,
       estimate = .pred_class)
```



#Random Forest
```{r}

```
#Ridge


```{r lr}
# create a linear_regression model so that you can tune the penalty parameter
# and use "glmnet" for the engine. If you set mixture = 1 for lasso regression above,
# what should you set mixture equal to for ridge regression here?
set.seed(20220502)
lr_mod <- logistic_reg() %>%
  set_engine("glm")%>%
  set_mode("classification")
# create a ridge workflow using your updated linear regression model you just created and
# the same recipe you defined above
lr_wf <- workflow() %>%
  add_recipe(theft_rec) %>%
  add_model(lr_mod)
lr_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
# perform hyperparameter tuning using the on your ridge hyperparameter grid and
# cross_validation folds you created above by modifying the line below
lr_res <- lr_wf %>%
  tune_grid(
    resamples = folds,
    grid = lr_grid,
    control=control_grid(save_pred = TRUE),
    metrics = metric_set(roc_auc))
  
lr_plot <- 
  lr_res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())

lr_plot 
# select the best model based on the "rmse" metric

top<-lr_res%>%
  show_best(metric="roc_auc", n=15)%>%
  arrange(penalty)
top

lr_best <- lr_res %>%
    select_best(metric="roc_auc")

lr_final <- finalize_workflow(
  lr_wf,
  parameters = lr_best
)

lr_wf_fit<-lr_final%>%
  fit(data=theft_train)

lr_fit<-lr_wf_fit%>%
  pull_workflow_fit()
# fit the final ridge model to the full training data and extract coefficients
# by updating the line below
lr_coefs <- lr_final %>%
  fit(data = theft_train) %>%
  extract_fit_parsnip() %>%
  vi(lambda = lr_best$penalty) 

vip(lr_fit)
```






```

#time series
```{r}
crimes_theft<-crimes_clean%>%
  select(offense, year, month, day, hours)%>%
  mutate(Date=make_datetime(year, month, day, hours))%>%
  select(offense,Date)%>%
  filter(offense=="Theft F/Auto")%>%
  group_by(Date)%>%
  dplyr::summarise(Theft=n())%>%
  ungroup()
head(crimes_theft,10) 
```
#Cross Validation
```{r}
#last 365 as testing data
theft_train <- head(crimes_theft, nrow(crime_theft)-365)
theft_test <- tail(crimes_theft, 365)

#creat object
theft_ts <- ts(theft_train$Theft, frequency = 24)

theft_plot <-theft_train %>%
ggplot(aes(x = Date, y = Theft)) +
geom_line(aes(color = "Theft")) +
scale_x_datetime(name = "Date", date_breaks = "5 year") +
scale_y_continuous(breaks = seq(0, 400, 100)) + 
theme_minimal() +
labs(title = "DC Theft Crime", subtitle = "2017 - 2021")
ggplotly(theft_plot)
```
```{r}
theft_ts_dec <- theft_ts %>%
  tail(365) %>%
  decompose()
theft_ts_dec %>%
  autoplot()
```

```{r}
# Create MSTS Object
theft_multi <- msts(theft_train$Theft, seasonal.periods = c(24, # Daily
                                                            24*7, # Weekly
                                                            24*30)) # Monthly

# Decompose MSTS Object
theft_multi_dec <- theft_multi %>%
  mstl()

theft_multi_dec %>%
  tail(365) %>%
  autoplot()
```
From the plot above, we can see the trend of the theft Crime is already going smooth. The theft Crime trend itself is decreasing in the last 365 days.
```{r}
# Create a data frame based on MSTS Object
theft_multi_df <- as.data.frame(theft_multi_dec)
p1 <- theft_multi_df %>%
  mutate(day = theft_train$Date) %>%
  group_by(day) %>%
  summarise(seasonal = sum(Seasonal24 + Seasonal168 + Seasonal720)) %>%
  head(24*2) %>%
  ggplot(aes(x = day, y = seasonal)) +
  geom_point(col = "red") + geom_line(col = "black") +
  theme_minimal()
p2 <- theft_multi_df %>%
  mutate(day = theft_train$Date) %>%
  group_by(day) %>%
  summarise(seasonal = sum(Seasonal24 + Seasonal168 + Seasonal720)) %>%
  head(24*7) %>%
  ggplot(aes(x = day, y = seasonal)) +
  geom_point(col = "red") + geom_line(col = "black") +
  theme_minimal()
p3 <- theft_multi_df %>%
  mutate(day = theft_train$Date, 
         month = month(theft_train$Date, label = T)) %>%
  group_by(month) %>%
  summarise(seasonal = sum(Seasonal24 + Seasonal168 + Seasonal720)) %>%
  head(24*30) %>%
  ggplot(aes(x = month, y = seasonal)) +
  geom_point() + geom_col() +
  theme_minimal()
(p1+p2)/p3
```
As we can see from the plot above, The crime tend to increase in the month of Feb, May, July, and Nov. These can be seen as a warning signal to people around the city that the chance of them getting harmed is high around these time.
#time series linear model
```{r}
set.seed(20220503)
ts_rec <- recipe(Theft ~., data = theft_train) %>%
   step_sqrt(all_outcomes()) %>% 
   step_center(all_outcomes()) %>%
   step_scale(all_outcomes()) %>%
   bake(prep(ts_rec, training = theft_train), new_data = theft_train)


```

#### Metro map 
```{r}
unzip(
  zipfile = "data/Metro_Stations_in_DC.zip",
  exdir = "data"
)
dc_stations <- st_read("data/Metro_Stations_in_DC.shp") %>%
  st_transform(crs = 4326)

ggplot() +
  geom_sf(data = dc_sf) +
  geom_sf(data = dc_stations) +
  theme_void()

boundary <- st_union(dc_sf) %>%
  st_sf(crs = 4326)

stations_filtered <- st_filter(dc_stations, boundary)

ggplot() +
  geom_sf(data = dc_sf) +
  geom_sf(data = stations_filtered) +
  theme_void()

stations_buffered <- stations_filtered %>%
  st_buffer(dist = units::set_units(0.25, "mile")) 

stations_buffered_joined <- st_join(stations_buffered, crimes_lim_sf, join = st_intersects)

stations_buffered_joined %>%
  as_tibble() %>%
  count(LONGNAME, sort = TRUE)
```
